{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Crime Classification and Description"
      ],
      "metadata": {
        "id": "xv6ik-B5Lxgi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H54WbEUhTxbj"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install tensorflow==2.12.0\n",
        "!pip install mediapipe\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For video loading and decoding\n",
        "!pip install decord av imageio"
      ],
      "metadata": {
        "id": "GlCf0-QcT-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face & Transformers\n",
        "!pip install transformers==4.40.1 timm accelerate\n",
        "# Optional: for newer vision models or FP16/Flash attention support\n",
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "FlnMYddbUBj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries"
      ],
      "metadata": {
        "id": "wOpxle-6Li07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "from PIL import Image\n",
        "from decord import VideoReader, cpu\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, LSTM, Dense, Dropout, LayerNormalization, Bidirectional, Lambda, Multiply)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab.patches import cv2_imshow\n",
        "import time"
      ],
      "metadata": {
        "id": "39_NfQxEUYpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification and description"
      ],
      "metadata": {
        "id": "Ss2TOAnbLqr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Force CPU usage\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "\n",
        "# ========== 🧠 Load Classifier ==========\n",
        "crime_classes = [\"Shoplifting\", \"Vandalism\"]\n",
        "\n",
        "def create_advanced_lstm_model(input_shape=(100, 225), num_classes=2):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = LSTM(64, return_sequences=True)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    attention_data = Dense(64, activation='tanh')(x)\n",
        "    attention_scores = Dense(1)(attention_data)\n",
        "    attention_scores = Lambda(lambda t: tf.nn.softmax(t, axis=1))(attention_scores)\n",
        "    x = Multiply()([x, attention_scores])\n",
        "    x = Lambda(lambda t: tf.reduce_sum(t, axis=1))(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dense(128, activation=\"swish\")(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(64, activation=\"swish\")(x)\n",
        "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = create_advanced_lstm_model()\n",
        "model.load_weights(\"/content/crime_detection_model_2class.h5\")\n",
        "# ========== 🧍‍♂️ Pose Extraction ==========\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic = mp_holistic.Holistic()\n",
        "\n",
        "def extract_pose_hand_landmarks(video_path, max_frames=100):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    skip = max(total_frames // max_frames, 1)\n",
        "    landmarks_list, count = [], 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        if count % skip == 0:\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = holistic.process(rgb)\n",
        "            data = []\n",
        "\n",
        "            if results.pose_landmarks:\n",
        "                data += [lm.x for lm in results.pose_landmarks.landmark]\n",
        "                data += [lm.y for lm in results.pose_landmarks.landmark]\n",
        "                data += [lm.z for lm in results.pose_landmarks.landmark]\n",
        "            else:\n",
        "                data += [0] * (33 * 3)\n",
        "\n",
        "            if results.left_hand_landmarks:\n",
        "                data += [lm.x for lm in results.left_hand_landmarks.landmark]\n",
        "                data += [lm.y for lm in results.left_hand_landmarks.landmark]\n",
        "                data += [lm.z for lm in results.left_hand_landmarks.landmark]\n",
        "            else:\n",
        "                data += [0] * (21 * 3)\n",
        "\n",
        "            if results.right_hand_landmarks:\n",
        "                data += [lm.x for lm in results.right_hand_landmarks.landmark]\n",
        "                data += [lm.y for lm in results.right_hand_landmarks.landmark]\n",
        "                data += [lm.z for lm in results.right_hand_landmarks.landmark]\n",
        "            else:\n",
        "                data += [0] * (21 * 3)\n",
        "\n",
        "            if len(data) == 225:\n",
        "                landmarks_list.append(data)\n",
        "        count += 1\n",
        "        if len(landmarks_list) >= max_frames: break\n",
        "    cap.release()\n",
        "\n",
        "    if len(landmarks_list) < max_frames:\n",
        "        landmarks_list += [[0] * 225] * (max_frames - len(landmarks_list))\n",
        "    return np.expand_dims(np.array(landmarks_list), axis=0)\n",
        "\n",
        "# Dummy crime classes and model (you should load your own trained model)\n",
        "crime_classes = [\"Shoplifting\", \"Vandalism\"]\n",
        "model = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(22500, 2), torch.nn.Softmax(dim=1))  # dummy model\n",
        "\n",
        "def predict_crime(video_path):\n",
        "    features = extract_pose_hand_landmarks(video_path)\n",
        "    features = torch.tensor(features, dtype=torch.float32)\n",
        "    pred = model(features)\n",
        "    return crime_classes[torch.argmax(pred).item()]\n",
        "\n",
        "# ========== 📜 InternVL Descriptor ==========\n",
        "os.environ[\"HF_TOKEN\"] = \"YOUR_HUGGINGFACE_TOKEN\"\n",
        "model_path = \"OpenGVLab/InternVL_2_5_HiCo_R16\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_auth_token=os.getenv(\"HF_TOKEN\"))\n",
        "intern_model = AutoModel.from_pretrained(model_path, torch_dtype=torch.float32, use_auth_token=os.getenv(\"HF_TOKEN\")).to(\"cpu\")\n",
        "intern_model = intern_model.to(torch.bfloat16)\n",
        "\n",
        "def build_transform(input_size=448):\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "def dynamic_preprocess(image, image_size=448):\n",
        "    return [image.resize((image_size, image_size))]\n",
        "\n",
        "def load_video_frames(video_path, input_size=448, num_segments=64):\n",
        "    vr = VideoReader(video_path, ctx=cpu(0))\n",
        "    max_frame = len(vr)\n",
        "    frame_indices = np.linspace(0, max_frame - 1, num_segments).astype(int)\n",
        "\n",
        "    transform = build_transform(input_size)\n",
        "    pixel_values_list = []\n",
        "    num_patches_list = []\n",
        "\n",
        "    for idx in frame_indices:\n",
        "        img = Image.fromarray(vr[idx].asnumpy())\n",
        "        processed_imgs = dynamic_preprocess(img, input_size)\n",
        "        pixels = [transform(tile) for tile in processed_imgs]\n",
        "        stacked = torch.stack(pixels)\n",
        "        pixel_values_list.append(stacked)\n",
        "        num_patches_list.append(stacked.shape[0])\n",
        "\n",
        "    pixel_values = torch.cat(pixel_values_list)\n",
        "    return pixel_values.to(torch.bfloat16), num_patches_list\n",
        "\n",
        "def generate_video_description(video_path, predicted_label):\n",
        "    prompts = {\n",
        "        \"Shoplifting\": \"Describe the suspicious activity in the video where a person is stealing an item.\",\n",
        "        \"Vandalism\": \"Describe the destructive activity where a person damages property.\"\n",
        "    }\n",
        "    pixel_values, num_patches_list = load_video_frames(video_path)\n",
        "    question = \"\".join([f\"Frame{i+1}: <image>\\n\" for i in range(len(num_patches_list))])\n",
        "    question += prompts[predicted_label]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output, _ = intern_model.chat(tokenizer, pixel_values, question, {\n",
        "            \"do_sample\": False, \"temperature\": 0.0, \"max_new_tokens\": 1024\n",
        "        }, num_patches_list=num_patches_list, return_history=True)\n",
        "    return output\n",
        "\n",
        "# ========== 🎬 Run Full Pipeline ==========\n",
        "def analyze_video(video_path):\n",
        "    predicted_label = predict_crime(video_path)\n",
        "    print(f\"🔹 Predicted Crime Class: {predicted_label}\")\n",
        "    description = generate_video_description(video_path, predicted_label)\n",
        "    print(f\"📝 Description: {description}\")\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        cv2.putText(frame, f\"Crime: {predicted_label}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
        "        if frame_count % 10 == 0:  # Show every 10th frame\n",
        "            cv2_imshow(frame)\n",
        "            time.sleep(0.05)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "# ========== 🔍 Run on Sample Video ==========\n",
        "if __name__ == \"__main__\":\n",
        "    analyze_video(\"/content/test_video_1.mp4\")"
      ],
      "metadata": {
        "id": "ji8sm0QbVwx1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}